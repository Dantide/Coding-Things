\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper,top=1cm,bottom=1cm,left=1cm,right=1cm,
marginpaperwidth=1cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{color}

% \setlength{\parskip}{1em} 
% \titlespacing*{\subsection}
%   {0pt}{3\baselineskip}{\baselineskip}

\def\columnseprulecolor{\color{black}}

\begin{document}

%Lecture 2 - Probability Law

\begin{multicols}{2}
\subsubsection{Additivity rules:}

For any events $A, B$:
\begin{equation}
  \tag{Event Union}
  \boxed{
    \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) -
    \mathbb{P}(A \cap B)
  }
\end{equation}

% Lecture 3 - Conditional Probability & Product Rule

\columnbreak
\subsubsection{Conditional Probability}

\begin{equation}
  \tag{Conditional Probability Def}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
  }
\end{equation}

If there is a finite number of different outcomes that are all equally
likely, the conditional prbability can be written as follows:

\begin{equation}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\text{number of elements of } A \cup
      B}{\text{number of elements of } B}
  }
\end{equation}
\end{multicols}

\subsubsection{Product Rule}

\begin{itemize}
\item If we have events $D_1$ to $D_n$ where $D_1 > D_2 > \dots > D_n$
  ($D_1$ largest, $D_n$ smallest), then we can apply the first form of
  the product rule:

  \begin{equation}
    \tag{Product Rule 1}
    \boxed{
      \mathbb{P}(D_n) = \mathbb{P}(D_1) \mathbb{P}(D_2 \mid D_1)
      \mathbb{P}(D_3 \mid D_2) \dots \mathbb{P}(D_n \mid D_{n-1}
    }
  \end{equation}

\item If we have events $A_1$ to $A_n$ with non-empty intersection
  (i.e. $A_1 \cap A_2 \cap \dots \cap A_n$), let $D_k = A_1 \cap A_2
  \cap \dots \cap A_k$, then $D_1 > D_2 > \dots > D_n$. If we then
  write the product rule on the events $D_n$ in terms of $A_n$, we
  get:

  \begin{equation}
    \tag{Product Rule 2}
    \boxed{
      \mathbb{P}(A_1 \cap \dots \cap A_n) = \mathbb{P}(A_1)
      \mathbb{P}(A_2 \mid A_1) \mathbb{P}(A_3 \mid A_1 \cap A_2) \dots
      \mathbb{P}(A_n \mid A_1 \cap \dots)
    }
  \end{equation}
\end{itemize}

% Lecture 4 - Total Probability Theorem

\subsubsection{Total Probability Theorem}

\begin{equation}
  \tag{Total Probability Theorem}
  \boxed{
    \mathbb{P}(B) = \mathbb{P}(B \mid C_1) \mathbb{P}(C_1) +
    \mathbb{P}(B \mid C_2) \mathbb{P}(C_2) + \dots + \mathbb{P}(B \mid
    C_n) \mathbb{P}(C_n)
  }
\end{equation}

% Lecture 5 - Bayes Law & Independence

\subsubsection{Bayes \& Independence}

If $A_1, A_2, \dots, A_n$ are events that partition $\Omega$, w/
nonzero $\mathbb{P}(A)$, then for any event $B$,

\begin{equation}
  \tag{Bayes' Law}
  \boxed{
    \mathbb{P}(A_k \mid B) =
    \frac{\mathbb{P}(B \mid A_k) \mathb{P}(A_k)}
    {\mathbb{P}(B \mid A_1) \mathbb{P}(A_1) + \dots + \mathbb{P}(B
      \mid A_n)}
  }
\end{equation}

\begin{equation}
  \tag{Independence Def}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B)
      &= \mathbb{P}(A) \mathbb{P}(B)
      && \text{or} \\
      \mathbb{P}(A \mid B)
      &= \mathbb{P}(A), \mathbb{P}(B) > 0
      && \text{or} \\
      \mathbb{P}(B \mid A)
      &= \mathbb{P}(B), \mathbb{P}(A) > 0
    \end{aligned}
  }
\end{equation}

% Lecture 6 - Conditional Dependence & Counting

\subsubsection{Conditional Dependence}

\begin{equation}
  \tag{Conditional Independence Def}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B \mid C)
      &= \mathbb{P}(A \mid C) \mathbb{P}(B \mid C)
      && \text{or} \\
      \mathbb{P}(A \mid B \cap C)
      &= \mathbb{P}(A \mid C), \mathbb{P}(B \cap C) > 0
    \end{aligned}
  }
\end{equation}

% Lecture 8 - Discrete Random Variables

\subsubsection{Discrete Random Variables}

\[
  \begin{aligned}
    p_X(x) \ge 0 & \quad \text{for every } x \in X \\
    \sum\limits_{x \in X} p_X(x) = 1 & \quad \text{(normalization)}
  \end{aligned}
\]

\begin{multicols}{2}
\begin{itemize}
\item \textbf{Discrete uniform pmf of interval $a \leq k \leq b$},
  $a, b \in \mathbb{N}$:
  
  \[
    p_X(k) =
    \begin{cases}
      \frac{1}{b - a + 1} & \text{when } a \leq k \leq b \\
      0 & \text{all over } k
    \end{cases}
  \]

\item Let $p \in [0,1]$; the \textbf{Bernoulli p pmf} be defined by:
  
  \[
    p_X(k) =
    \begin{cases}
      p & \text{when } k = 1 \\
      1 - p & \text{when } k = 0
    \end{cases}
  \]

\columnbreak
    
\item Given positive integer $n$, some $p \in [0,1]$, the
  \textbf{Binomial(n,p) pmf} is defined as:

  \[
    p_X(k) = {n \choose k} p^k (1 - p)^{n-k} \quad 0 \leq k \leq n
  \]

\item Given $p \in (0, 1)$ the \textbf{geometric pmf} defined by:

  \[
    p_X(k) = p (1-p)^{k-1} \quad \text{for all } 1 \leq k \leq \infty
    \text{positive integers}
  \]

\item Poisson(X):

  \[
    p_X(k) = e^{-\lambda} \frac{\lambda^{k}}{k!} \quad 0 \leq k \leq
    \infty (k \in \mathbb{N})
  \]
\end{itemize}
\end{multicols}

% Lecture 9 - Expectation, Variance

\subsubsection{Expectation, Variance}

\begin{equation}
  \tag{Expected Value Definition}
  \boxed{
    \mathbb{E}(X) = \sum\limits_{x \in X} x p_X(x)
  }
\end{equation}

\begin{equation}
  \tag{Expected Value Rule}
  \boxed{
    \mathbb{E}(Y) = \sum\limits_{x \in X} g(X) p_X(x)
  }
\end{equation}


\end{document}