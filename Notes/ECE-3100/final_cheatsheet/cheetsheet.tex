\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper,top=1cm,bottom=1.25cm,left=1cm,right=1cm,
marginpaperwidth=1cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{color}
\usepackage{comment}

% \setlength{\parskip}{1em} 
% \titlespacing*{\subsection}
%   {0pt}{3\baselineskip}{\baselineskip}

\def\columnseprulecolor{\color{black}}

\begin{document}


%Lecture 2 - Probability Law

\begin{multicols}{2}
\subsubsection*{Additivity rules:}

For any events $A, B$:
\begin{equation*}
  \boxed{
    \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) -
    \mathbb{P}(A \cap B)
  }
\end{equation*}

\subsubsection*{Counting}

\begin{equation*}
  \boxed{
    \text{comb:} &= \frac{n!}{(n-k)!} \ 
    \text{perm:} &= \frac{n!}{k!(n-k)!} = {n \choose k}
  }
\end{equation*}

% Lecture 3 - Conditional Probability & Product Rule

\columnbreak
\subsubsection*{Conditional Probability}

\begin{equation*}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
  }
\end{equation*}

\begin{equation*}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B \mid C)
      &= \mathbb{P}(A \mid C) \mathbb{P}(B \mid C)
      && \text{or} \\
      \mathbb{P}(A \mid B \cap C)
      &= \mathbb{P}(A \mid C), \mathbb{P}(B \cap C) > 0
    \end{aligned}
  }
\end{equation*}
\end{multicols}

\subsubsection*{Product Rule}

\begin{itemize}
\item With events $D_1$ to $D_n$ where $D_1 > D_2 > \dots > D_n$
  ($D_1$ largest, $D_n$ smallest):

  \begin{equation}
    \tag{Product Rule 1}
    \boxed{
      \mathbb{P}(D_n) = \mathbb{P}(D_1) \mathbb{P}(D_2 \mid D_1)
      \mathbb{P}(D_3 \mid D_2) \dots \mathbb{P}(D_n \mid D_{n-1}
    }
  \end{equation}

\item With events $A_1$ to $A_n$ with non-empty intersection, let
  $D_k = A_1 \cap A_2 \cap \dots \cap A_k$, then
  $D_1 > D_2 > \dots > D_n$:

  \begin{equation}
    \tag{Product Rule 2}
    \boxed{
      \mathbb{P}(A_1 \cap \dots \cap A_n) = \mathbb{P}(A_1)
      \mathbb{P}(A_2 \mid A_1) \mathbb{P}(A_3 \mid A_1 \cap A_2) \dots
      \mathbb{P}(A_n \mid A_1 \cap \dots)
    }
  \end{equation}
\end{itemize}

% Lecture 4 - Total Probability Theorem

\subsubsection*{Total Probability Theorem in Bayes, \& Independence}

% Lecture 5 - Bayes Law & Independence

\begin{multicols}{2}

\begin{equation*}
  \boxed{
    \mathbb{P}(A_k \mid B) =
    \frac{\mathbb{P}(B \mid A_k) \mathb{P}(A_k)}
    {\mathbb{P}(B \mid A_1) \mathbb{P}(A_1) + \dots + \mathbb{P}(B
      \mid A_n) \text{or} \mathbb{P}(B)}
  }
\end{equation*}

\columnbreak

\begin{equation*}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B)
      &= \mathbb{P}(A) \mathbb{P}(B)
      && \text{or} \\
      \mathbb{P}(A \mid B)
      &= \mathbb{P}(A), \mathbb{P}(B) > 0
    \end{aligned}
  }
\end{equation*}
\end{multicols}

\begin{multicols}{2}
\subsubsection*{Covariance}
\begin{equation*}
  \boxed{
    \begin{aligned}
      Cov(X,Y) &= \mathbb{E}((X - \mathbb{E}(X))(Y-\mathbb{E}(Y))) \\
      \rho &= \frac{Cov(X,Y)}{\sigma_X \sigma_Y} \\
      Cov = 0 &\Rightarrow Var(X + Y) = Var(X) + Var(Y) \\
      \text{indep} &\Rightarrow Cov = 0
    \end{aligned}
  }
\end{equation*}

\columnbreak
\subsubsection*{Pmfs}

\begin{equation*}
  \boxed{
    \begin{aligned}
      p_{X,Y} (x,y) &= \mathbb{P}(\{X=x\} \cap \{Y=y\}) \\
      \mathbb{E}(Z) &= \sum_{x\in X} \sum_{y\in Y} g(x,y) p_{X,Y} (x,y) \\
      p_{X\mid A}(x) &= \frac{\mathbb{P}(\{X = x\} \cap A)}{\mathbb{P}(A)}
      \quad
      p_{X\mid Y}(x\mid y) = \frac{p_{X,Y}(x,y)}{p_Y(y)}
    \end{aligned}
  }
\end{equation*}

\end{multicols}

% Lecture 8 - Discrete Random Variables

\subsubsection*{Discrete Random Variables}

\begin{multicols}{2}
\begin{itemize}
% \item \textbf{Discrete uniform pmf of interval $a \leq k \leq b$},
%   $a, b \in \mathbb{N}$:
  
%   \[
%     p_X(k) =
%     \begin{cases}
%       \frac{1}{b - a + 1} & \text{when } a \leq k \leq b \\
%       0 & \text{all over } k
%     \end{cases}
%   \]

\item Let $p \in [0,1]$; the \textbf{Bernoulli p pmf} be defined by:
  
  \[
    p_X(k) =
    \begin{cases}
      p & \text{when } k = 1 \\
      1 - p & \text{when } k = 0
    \end{cases}

    \mathbb{E}(X) = p; Var(X) = p(1 - p)
  \]
    
\item Given positive integer $n$, some $p \in [0,1]$, the
  \textbf{Binomial(n,p) pmf} is defined as:

  \[
    p_X(k) = {n \choose k} p^k (1 - p)^{n-k} \quad 0 \leq k \leq n
  \]

% \item Given $p \in (0, 1)$ the \textbf{geometric pmf} defined by:

%   \[
%     p_X(k) = p (1-p)^{k-1} \quad \text{for all } 1 \leq k \leq \infty
%     \text{positive integers}
%   \]

\item Poisson(X):

  \[
    p_X(k) = e^{-\lambda} \frac{\lambda^{k}}{k!} \quad 0 \leq k \leq
    \infty (k \in \mathbb{N})

    \mathbb{E}(X) = \lambda; Var(X) = \lambda
  \]
\end{itemize}
\end{multicols}

% Lecture 9 - Expectation, Variance

\subsubsection*{Expectation, Variance}

\begin{multicols}{2}
\begin{equation*}
  \boxed{
    \begin{aligned}
      \mathbb{E}(X) &= \sum\limits_{x \in X} x p_X(x) \\
      \mathbb{E}(Y) &= \sum\limits_{x \in X} g(X) p_X(x) \\
      % continuuous
      \mathbb{E}(X\mid A) &= \int_{-\infty}^\infty xf_{X\mid A}(x) dx \\
      \mathbb{E}(X\mid Y=y) &= \int _{-\infty}^\infty xf_{X\mid
        Y}(x\mid y) dx \forall y \\
      \mathbb{E}(X) &= \mathbb{E}(\mathbb{E}(X\mid Y))
    \end{aligned}
  }
\end{equation*}

\columnbreak

\begin{equation*}
  \boxed{
    \begin{aligned}
      Var(X) &= \mathbb{E}((X - \mathbb{E}(X))^2) \\
      Var(X) &= \mathbb{E}(X^2) - (\mathbb{E}(X))^2 \\
      \sigma_X &= \sqrt{Var(X)} \\
      Var(X) &= \mathbb{E}(Var(X\mid Y)) + Var(\mathbb{E}(X\mid Y)) \\
      % continuous
      Var(X) &= \mathbb{E}[(X - \mathbb{E}[X])^2] \\
      Var(X) &= \int_{-\infty}^\infty (X - \mathbb{E})^2 f_X(x) dx \\
      Var(X\mid Y) &= \mathbb{E}((X - \mathbb{E}(X\mid Y))^2\mid Y)
    \end{aligned}
  }
\end{equation*}
\end{multicols}












\pagebreak

\begin{multicols}{2}
\subsubsection*{Pdf, Cdf Def}

\begin{equation*}
  \boxed{
    \begin{aligned}
      \mathbb{P}(a\leq X\leq b) &= \int_a^b f_X(x)dx  &&
      \begin{aligned}
        F_X(x) &= \int_{-\infty}^x f_X(t)dt \text{ \underline{\textbf{cont}}} \\
        f_X(x) &= \frac{d}{dx} F_X(x)
      \end{aligned} \\
      \mathbb{P}(\{X\in V\}) &= \int_V f_X(x)dx &&
      \begin{aligned}
        F_X(x) &= \mathbb{P}(\{X\leq x\}) \text{ \underline{\textbf{discr}}} \\
        F_X(x) &= \sum_{\{x_k \mid x_k \leq x\}} p_X(x_k)
      \end{aligned} \\
      p_X(x_k) &= F_X(x_k) - F_X(x_{k-1})
    \end{aligned}
  }
\end{equation*}


\columnbreak
\subsubsection*{\qquad Mean and Variance: Continuous}
\begin{equation*}
  \boxed{
    \mathbb{E}(X) = \int_{-\infty}^{+\infty} xf_X(x)dx
  }
\end{equation*}

\begin{equation*}
  \boxed{
    Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
  }
\end{equation*}
\end{multicols}

\subsubsection*{Types of continuous rvs}

\begin{multicols}{2}
\begin{itemize}
\item \textbf{X uniform on [a,b]:}
  \begin{equation*}
    \boxed{
      \begin{aligned}
        f_X(x) &=
        \begin{cases}
          \frac{1}{b-a} & \text{when } x \in [a,b] \\
          0 & \text{else}
        \end{cases} \\
        F_X(x) &=
        \begin{cases}
          0 & \text{when } x < a \\
          \frac{x-a}{b-a} & \text{when } a\leq x\leq b \\
          1 & \text{when } x > b
        \end{cases} \\
        \mathbb{E}[X] &= \frac{b+a}{2} \qquad \qquad
        Var(X) = \frac{(b-a)^2}{12}
      \end{aligned}
    }
  \end{equation*}

% \item \textbf{X exponential($\lambda$):}
%   \begin{equation*}
%     \boxed{
%       \begin{aligned}
%         f_X(x) &=
%         \begin{cases}
%           \lambda e^{-\lambda x} & \text{when } x \geq 0 \quad \forall
%           \lambda > 0 \\
%           0 & \text{when } x < 0
%         \end{cases} \\
%         F_X(x) &=
%         \begin{cases}
%           1 - e^{-\lambda x} & \text{when } x\geq 0 \\
%           0 & \text{otherwise}
%         \end{cases} \\
%         \mathbb{E}[X] &= \frac{1}{\lambda} \qquad \qquad
%         Var(X) = \frac{1}{\lambda^2}
%       \end{aligned}
%     }
%   \end{equation*}

% \item \textbf{Gaussian rv:}
%   \begin{equation*}
%     \boxed{
%       \begin{aligned}
%         f_X(x) &= \frac{1} {\sqrt{2x\sigma^2}}
%         \exp{\Big(-\frac{(x-\mu)^2} {2\sigma^2}\Big)} \\
%         F_X(x) &= \frac{1} {\sqrt{2x\sigma^2}} \int_{-\infty}^x
%         \exp{\Big(-\frac{(t-\mu)^2}{2\sigma^2}\Big)} dt \\
%         \mathbb{E}(X) &= \mu \qquad \qquad
%         Var(X) = \sigma^2
%       \end{aligned}
%     }
%   \end{equation*}

\item \textbf{standard normal:}

  \begin{equation*}
    \boxed{
      \begin{aligned}
        f_X(x) &= \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \qquad 
        \therefore \mathbb{E}(X)=0, \ Var=1 \\
        \Phi(x) &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt \\
        \mathbb{P}(\{X>x\}) &= \mathbb{P}(\{\sigma + Y + M > x\}) =
        \mathbb{P}\Big(Y>\frac{x-M}{\sigma}\Big)
      \end{aligned}
    }
  \end{equation*}
\end{itemize}
\end{multicols}

\subsubsection*{Joint Pdf}

\begin{equation*}
  \boxed{
    \begin{aligned}
      \begin{aligned}
        \mathbb{P}(\{(X,Y)\in V\}) &= \iint_{-V} f_{X,Y}(x,y) dx dy \\
        \mathbb{E}[g(X,Y)] &= \int_{-\infty}^{+\infty}
        \int_{-\infty}^{+\infty} g(X,Y) f_{X,Y}(x,y) dx dy \\
        F_{X,Y}(x,y) &= \mathbb{P}(X\leq x, Y\leq y)
      \end{aligned} \quad &
      \begin{aligned}
        f_X(x) &= \int_{-\infty}^{+\infty} f_{X,Y}(x,y) dy \\
        f_Y(y) &= \int_{-\infty}^{+\infty} f_{X,Y}(x,y) dx \\
        f_{X,Y}(x,y) &= \frac{\delta F_{X,Y}} {\delta x\delta y}(x,y)
      \end{aligned}
    \end{aligned}
  }
\end{equation*}

\subsubsection*{Indpendence: Continuous}

\begin{multicols}{2}
\begin{itemize}
\item $f_{X,Y}(x,y) = f_X(x)f_Y(y) \quad \forall x, y$
\item $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$
\item $\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))$
\item $Var(X+Y) = Var(X)+Var(Y)$
\end{itemize}
\end{multicols}

\begin{multicols}{2}
\subsubsection*{Conditioning on Event}

\begin{equation*}
  \boxed{
    \begin{aligned}
      \mathbb{P}(\{X\in V\}\mid A) &= \int_V f_{X\mid A}(x) dx \\
      f_{X\mid A}(x) &=
      \begin{cases}
        \frac{f_X(x)} {\mathbb{P}(\{X\in W\})} & \text{when } X\in W \\
        0 & \text{otherwise}
      \end{cases} \\
      \mathbb{E}(X \mid A) &= \int_{-\infty}^{+\infty} xf_{X\mid A}(x)dx \\
      \mathbb{E}(X \mid A) &= \sum_{x\in X} xp_{X\mid A}(x)
    \end{aligned}
  }
\end{equation*}

\subsubsection*{Conditioning on rv}

\begin{equation*}
  \boxed{
    \begin{aligned}
      f_{X\mid Y}(x\mid y) &= \frac{f_{X,Y}(x,y)}{f_Y(y)} \\
      f_X(x) &= \int_{-\infty}^{+\infty} f_Y(y)f_{X\mid Y}(x\mid y) dy \\
      \mathbb{E}[X\mid Y=y] &= \int_{-\infty}^{+\infty} xf_{X\mid Y}(x\mid y) dx
    \end{aligned}
  }
\end{equation*}
\end{multicols}

\subsubsection*{Inequals, Moment Functions, Limit Theorems}

\begin{equation*}
  \boxed{
    \begin{aligned}
      \begin{aligned}
        \mathbb{P}(\{|X-\mu | \geq c\}) \leq \frac{Var(X)}{c^2} \\
        \mathbb{P}(\{X\geq c\}) \leq \frac{\mathbb{E}(X)}{c} \\
      \end{aligned}
      \qquad &
      \begin{aligned}
        M_X(s) &= \mathbb{E}(e^{sX}) \\
        M_{\alpha Y+\beta} &= e^{\beta s}M_Y(\alpha s) \\
      \end{aligned}
      \qquad &&
      \begin{aligned}
        S_n &= X_1 + \dots + X_n \text{iid}
        &&\mathbb{E}(S_n) = n \mu \qquad Var(S_n)=n\sigma^2 \\
        M_n &= \frac{1}{n} S_n
        &&\mathbb{E}(M_n) = \mu \qquad Var(M_n)=\frac{\simga^2}{n} \\
        Z_n &= \frac{X_1 +\dots+ X_n -n\mu}{\sqrt{n}\sigma}
        \quad &&\mathbb{E}(Z_n) = 0 \qquad Var(Z_n) = 1
      \end{aligned}
    \end{aligned}
  }
\end{equation*}

\end{document}