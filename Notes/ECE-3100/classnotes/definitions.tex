\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper,top=3cm,bottom=2cm,left=3cm,right=3cm,
marginpaperwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{titlesec}

% \setlength{\parskip}{1em} 
\titlespacing*{\subsection}
  {0pt}{3\baselineskip}{\baselineskip}

\title{ECE 3100 - Functions, Formulas, and Definitions}
\author{Stephen Chin}
\date{Spring Semester 2019}


\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Pre - Prelim 1}

\subsection{Lecture 1 - What is Probability?}

\textbf{Probability} is a way of mathematically modelling situations
involving uncertainty with the goal of making predications decisions
and models. Probability can be understood in many ways, such as:

\begin{enumerate}
\item Frequency of Occurence: Or percentage of successes in a
  moderately large number of similar situations.
  
\item Subjective belief: Or ceratinty based on other understood facts
  about a claim.
\end{enumerate}

For our Probability Models, we define the set of all outcomes to be
$\Omega$, better known as the \textbf{sample space} of an
experiment. All subsets of $\Omega$ are called \textbf{events}. These
are both sets and can be understood using default set notation.


\medskip \hrule
\subsection{Lecture 2 - Probability Law}

Given $\Omega$ chosen, a \textbf{probability law} on $\Omega$ is a
mapping $\mathbb{P}$ that assings a number for every event such that:

\begin{equation} \tag{Kolmogorov's Axioms} \boxed{
    \begin{aligned} \mathbb{P}(A) \ge 0 & \quad \text{for every event
        A} \\ \mathbb{P}(\Omega) = 1 & \quad \text{(normalization)}
    \end{aligned} }
\end{equation}

\bigskip
\subsubsection{Additivity rules:}

\begin{itemize}
\item If $A \cap B = \varnothing$, ($A, B$) events, then:
  \begin{equation}
    \boxed{
      \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B)
    }
  \end{equation}

\item If events $A_1, A_2, \dots$ are all disjoint, then:
  \begin{equation}
    \boxed{
      \mathbb{P} (\bigcup\limits_{n=1}^{\infty} A_n) =
      \sum_{n=1}^{\infty} \mathbb{P}(A_n)
    }
  \end{equation}
\end{itemize}

By these rules, we can surmise that $\boxed{\mathbb{P} (\varnothing) =
  0}$.

For any events $A, B$:
\begin{equation}
  \tag{Event Union}
  \boxed{
    \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) -
    \mathbb{P}(A \cap B)
  }
\end{equation}

When we have a probability law on a finite $\Omega$ with all outcomes
equally likely (i.e. $\mathbb{P}(\{s\}) = 1/size(\Omega)$), we call
this probability law $\mathbb{P}$ a \textbf{(discrete) uniform
  probability law}.


\medskip \hrule
\subsection{Lecture 3 - Conditional Prob \& Product Rule}

\subsubsection{Conditional Probability}

\textbf{Conditional Probability} is defined $\mathbb{P}(A \mid B)$ =
``Probability of A given B''. It is understood as the likelyhood that
event A occurs, given that B also occurs. 

\begin{equation}
  \tag{Conditional Probability Def}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
  }
\end{equation}

If there is a finite number of different outcomes that are all equally
likely, the conditional prbability can be written as follows:

\begin{equation}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\text{number of elements of } A \cup
      B}{\text{number of elements of } B}
  }
\end{equation}

\subsubsection{Product Rule}

There are two main ways to write the product rule and they both have
different setups.

\begin{itemize}
\item If we have events $D_1$ to $D_n$ where $D_1 > D_2 > \dots > D_n$
  ($D_1$ largest, $D_n$ smallest), then we can apply the first form of
  the product rule:

  \begin{equation}
    \tag{Product Rule 1}
    \boxed{
      \mathbb{P}(D_n) = \mathbb{P}(D_1) \mathbb{P}(D_2 \mid D_1)
      \mathbb{P}(D_3 \mid D_2) \dots \mathbb{P}(D_n \mid D_{n-1})
    }
  \end{equation}

\item If we have events $A_1$ to $A_n$ with non-empty intersection
  (i.e. $A_1 \cap A_2 \cap \dots \cap A_n$), let $D_k = A_1 \cap A_2
  \cap \dots \cap A_k$, then $D_1 > D_2 > \dots > D_n$. If we then
  write the product rule on the events $D_n$ in terms of $A_n$, we
  get:

  \begin{equation}
    \tag{Product Rule 2}
    \boxed{
      \mathbb{P}(A_1 \cap \dots \cap A_n) = \mathbb{P}(A_1)
      \mathbb{P}(A_2 \mid A_1) \mathbb{P}(A_3 \mid A_1 \cap A_2) \dots
      \mathbb{P}(A_n \mid A_1 \cap \dots)
    }
  \end{equation}
\end{itemize}


\medskip\hrule
\subsection{Lecture 5 - Bayes Law \& Independence}

\subsubsection{Bayes Law}

\textbf{Bayes' Rule} is defined by mixing the defintion of Condition
Probability, and the Total Probability Theorem.

Given $\Omega, \mathbb{P}$, if $A_1, A_2, \dots, A_n$ are events that
partition $\Omega$, and have nonzero $\mathbb{P}(A)$, then for any
event $B$,

\begin{equation}
  \tag{Bayes' Law}
  \boxed{
    \mathbb{P}(A_k \mid B) =
    \frac{\mathbb{P}(B \mid A_k) \mathb{P}(A_k)}
    {\mathbb{P}(B \mid A_1) \mathbb{P}(A_1) + \dots + \mathbb{P}(B
      \mid A_n)}
  }
\end{equation}

\subsubsection{Indpendence}

Given $\Omega, \mathbb{P}$, any events $A$ and $B$ are
\textbf{independent} when:

\begin{equation}
  \tag{Independence Def}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B)
      &= \mathbb{P}(A) \mathbb{P}(B)
      && \text{or} \\
      \mathbb{P}(A \mid B)
      &= \mathbb{P}(A), \mathbb{P}(B) > 0
    \end{aligned}
  }
\end{equation}


\medskip\hrule
\subsection{Lecture 6 - Conditional Dependence \& Counting}

\subsubsection{Conditional Dependence}

Given $\Omega$ and $\mathbb{P}$: say that events $A$ and $B$ are
\textbf{conditionally independent} given $C$ when:

\begin{equation}
  \tag{Conditional Independence Def}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B \mid C)
      &= \mathbb{P}(A \mid C) \mathbb{P}(B \mid C)
      && \text{or} \\
      \mathbb{P}(A \mid B \cap C)
      &= \mathbb{P}(A \mid C), \mathbb{P}(B \cap C) > 0
    \end{aligned}
  }
\end{equation}

\subsubsection{Counting}

\textbf{Counting} is the process of using the number of elements in
the events to calculate probability. This technique mostly arises in
situations where either:

\begin{itemize}
\item The sample space $\Omega$ has a finite number of equally likely
  outcomes. Then, for any event $A$,

  \[
    \mathbb{P}(A) = \frac{\text{\# of elements of } A}
    {\text{\# of elements of } \Omega}
  \]

\item An event $A$ has a finite number of equally likely outcomes with
  probability $p$. Then for that event $A$:

  \[
    \mathbb{P}(A) = p \cdot (\text{\# of elements of }A)
  \]
\end{itemize}


\medskip\hrule
\subsection{Lecture 7 - Counting}

\textbf{Counting Principle:} in a process with a sequence of stages
$1, 2, \dots, r$ with $n_1$ choices at stage 1 over to $n_r$ at stage
$r$; \# of coutcomes is $n_1 n_2 \dots n_k$.

Can be used to rederive (\# subsets of $\Omega$) = $2^{\#(elem)}$.

\bigskip
\subsubsection{$k$-permutations of $n$ objects}

We are given $n$ distinct objects and a number $k \leq n$, and we want
to find out the number of ways we could take $k$ distinct objects from
the group of $n$ objects and arrange them in a sequence. By using the
Counting Principle, we can find that the \textbf{number of
  k-permutations} of this set is:

\begin{equation}
  \tag{K-permutations}
  \boxed{
    \begin{aligned}
      n (n-1) \dots (n-k+1)
      &= \frac{n (n-1) \dots (n-k+1) (n-k) \dots 2 \cdot 1}{(n-k)
        \dots 2 \cdot 1} \\
      &= \frac{n!}{(n-k)!}
    \end{aligned}
  }
\end{equation}

Special Case: If $k = n$, then the number of $k$-permutations of $n$
objects is simply $n!$.

\bigskip
\subsubsection{$k$-combinations of $n$ objects}

For finding the number of $k$-combinations, we can look back to our
$k$-permutations and reason about them. Say we have the same setup as
before but we are not arranging the items in a sequence. For each
combination, we have $k!$ ``duplicate'' permutations. Thus, we can
look at the number permutations and reason that the number of
k-combinations should be that over $k!$, making the \textbf{number of
  k-combinations} of this set is:

\begin{equation}
  \tag{K-combinations}
  \boxed{
    \frac{n!}{k!(n-k)!} = \binom{n}{k}
  }
\end{equation}


\medskip\hrule
\subsection{Lecture 8 - Discrete Random Variables}

\subsubsection{Random Variables}

Given $\Omega$ and $\mathbb{P}$, a \textbf{discrete random variable
  (r.v.)} is a real valued function with domain $\Omega$ that takes on
only finite or countably infinite number of different values (i.e. $X
: \Omega \rightarrow \mathbb{R}$).

\subsubsection{Probability Mass Functions}

Given $\Omega, \mathbb{P}$, associated with any discrete rv $X :
\Omega \rightarrow \mathbb{R}$ is $X$'s \textbf{probability mass
  function (pmf)} - notation $p_X$.

\begin{equation}
  \tag{pmf Def}
  \boxed{
    \forall x \text{ of } X, p_X(x) = \mathbb{P}(A_X)
    \text{ where } A_X = \{s \in \Omega : X(s) = x\}
  }
\end{equation}

\bigskip
\underline{Things to Note:}

\begin{itemize}
\item $\mathbb{P}(A_X)$ can also be written as $\mathbb{P}(\{X = x\})$
  or $\mathbb{P}(X = x)$.

\item $p_X(x) \geq 0$ for all possible values of $X$.

\item A pmf is essentially a probability law on the different values
  in the codomain of a random variable, so the same laws that apply
  to probability laws apply to pmfs:

  \[
    \begin{aligned}
      p_X(x) \ge 0 & \quad \text{for every } x \in X \\
      \sum\limits_{x \in X} p_X(x) = 1 & \quad \text{(normalization)}
    \end{aligned}
  \]

\item If $V$ is any finite or countably inf. set of possible values of
  $X$, then if we set $B = \{ $the event ``$X \in V$''$\}$,
  (i.e. $B = \{ s \in \Omega : X(s) \in V \} $), then
  $\mathbb{P}(B) = \sum\limits_{x \in V} p_X(x)$.
\end{itemize}

Note: for a given pmf, there are multiple $\Omega$'s, $\mathbb{P}$'s,
$X$'s that lead to that PMF.

\subsubsection{Common PMFs}

\begin{itemize}
\item \textbf{Discrete uniform pmf of interval $a \leq k \leq b$},
  $a, b \in \mathbb{N}$:
  
  \[
    p_X(k) =
    \begin{cases}
      \frac{1}{b - a + 1} & \text{when } a \leq k \leq b \\
      0 & \text{all over } k
    \end{cases}
  \]

\item Let $p \in [0,1]$; the \textbf{Bernoulli p pmf} be defined by:
  
  \[
    p_X(k) =
    \begin{cases}
      p & \text{when } k = 1 \\
      1 - p & \text{when } k = 0
    \end{cases}
  \]
    
\item Given positive integer $n$, some $p \in [0,1]$, the
  \textbf{Binomial(n,p) pmf} is defined as:

  \[
    p_X(k) = {n \choose k} p^k (1 - p)^{n-k} \quad 0 \leq k \leq n
  \]

  This pmf tends to show up in situations involving sequences of
  independent trials, such as coin flips. Useful if you are trying to
  find the probability of $k$ heads in $n$ coin flips.

\item Given $p \in (0, 1)$ the \textbf{geometric pmf} defined by:

  \[
    p_X(k) = p (1-p)^{k-1} \quad \text{for all } 1 \leq k \leq \infty
    \text{ positive integers}
  \]

  This pmf tends to show up in situations such as $\mathbb{P}$(it
  takes $k$ flips to flip a heads).

\item Poisson(X):

  \[
    p_X(k) = e^{-\lambda} \frac{\lambda^{k}}{k!} \quad 0 \leq k \leq
    \infty (k \in \mathbb{N})
  \]
\end{itemize}


\medskip\hrule
\subsection{Lecture 9 - Expectation, Variance}

\subsubsection{Function of a Random Variable}

Given a random variable $X$ and any function $g : \mathbb{R}
\rightarrow \mathbb{R}$, can define another r.v. $Y = g(x)$:

\[
  \forall s \in \Omega, Y(s) = g(X(s))
\]

The function of a discrete r.v. is another discrete r.v.. Generally,
it is non-trivial to get the pmf of $Y=g(X)$, but it is sometimes
easy. (See examples)

\subsubsection{Expected Value}

Given a discrete r.v. $X$ with $p_X(x)$ pmf, we define the
\textbf{expected value (expectation)}:

\begin{equation}
  \tag{Expected Value Definition}
  \boxed{
    \mathbb{E}(X) = \sum\limits_{x \in X} x p_X(x)
  }
\end{equation}

Given $X, Y = g(X)$, what is $\mathbb{E}(Y)$? One way is to figure out
$p_Y(g)$ for all possible values of $y \in Y$ and then find it
through:

\[
  \mathbb{E}(Y) = \sum\limits_{y \in Y} y p_Y(y)
\]

and get $P_y$ through $p_x$, though that is generally a non-trivial
solution. However, another possible solution is to use the
\textbf{Expected Value Rule}.

\subsubsection{Expected Value Rule}

Given $X, p_X,$ and $Y=g(X)$,

\begin{equation}
  \tag{Expected Value Rule}
  \boxed{
    \mathbb{E}(Y) = \sum\limits_{x \in X} g(X) p_X(x)
  }
\end{equation}

Special Case: $Y = \alpha X + \beta$

\[
  \begin{aligned}
    \mathbb{E}(Y)
    &= \sum\limits_{x \in X} g(x) p_X(x) \\
    &= \sum\limits_{x \in X} (\alpha x + \beta) p_X(x) \\
    &= \alpha \sum\limits_{x \in X} x p_X(x) + \beta \sum\limits_{x
      \in X} p_X(x) \\
    &= \alpha \mathbb{E}(X) + \beta    
  \end{aligned}
\]

\subsubsection{Variance}

Given an rv $X$ with pmf $p_X$, we define \textbf{Variance} to be:

\begin{equation}
  \tag{Variance Def}
  \boxed{
    Var(X) = \mathbb{E}((X - \mathbb{E}(X))^2)
  }
\end{equation}

Off of this definition, we also define standard deviation to be
$\sigma_X = \sqrt{Var(X)}$.


\medskip\hrule
\subsection{Lecture 10 - Expected Value and Variance Examples}

\begin{itemize}
\item $X$ is \textbf{Bernoulli(p)}:

  \[
    \mathbb{E}(X) = p; Var(X) = p(1 - p)
  \]

\item $X$ is \textbf{discrete uniform on $a \le k \le b$}:

  \[
    \mathbb{E}(X) = \frac{b + a}{2}
  \]
  
\item $X$ is \textbf{Poisson($\lambda$)}:

  \[
    \mathbb{E}(X) = \lambda; Var(X) = \lambda
  \]

\item $X$ is \textbf{Geometric(p)}:

  \[
    \mathbb{E}(X) = \frac{1}{p}
  \]
\end{itemize}





\medskip\hrule
\pagebreak
\section{Post Prelim 1 - Pre Prelim 2}

\subsection{Lecture 11 - Multiple Discrete RVs, Joint pmf's,
  Conditionals}

\subsubsection{Joint pmf's}

Given $\Omega, \mathbb{P}$, and two discrete rv's $X, Y$ defined in
$\Omega$, define the \textbf{joint pmf} of $X \& Y$ as:

\begin{equation}
  \tag{Joint pmf}
  \boxed{
    \forall x \in X, y \in Y, \quad p_{X,Y} (x,y) = \mathbb{P}(\{X=x\} \cap
    \{Y=y\}) = \mathbb{P}(A_x \cap B_y)
  }
\end{equation}

For any set $V$ of possible value pairs $x,y$, we have that

\[
  \sum_{x,y \in V} p_{X,Y} (x,y) = \mathbb{P}(\text{event that} (X,Y)
  \in V)
\]

From the joint pmf $p_{X,Y}(x,y)$, we can derive the \textbf{marginal
  pmfs} $p_X(x)$ and $p_Y(y)$ from the joint pmf as:

\begin{equation}
  \tag{Marginal Pmfs}
  \boxed{
    \begin{split}
      \forall x, \quad p_X(x) &= \sum_y p_{X,Y}(x,y) \\
      \forall y, \quad p_Y(y) &= \sum_x p_{X,Y}(x,y) 
    \end{split}
  }
\end{equation}

Given $X,Y$ with joint pmf $p_{X,Y}(x,y)$ and some real valued
function $Z=g(X,Y)$, we have:

\begin{equation}
  \tag{Joint Expected Value Rule}
  \boxed{
    \mathbb{E}(Z) = \sum_{x\in X} \sum_{y\in Y} g(x,y) p_{X,Y} (x,y)
  }
\end{equation}

Special g choice: $g(X,Y) = \alpha X + \beta Y = \gamma$

\[
  \mathbb{E}(z) = \sum_x \sum _y (\alpha x + \beta y + \gamma) p_{X,Y}
  (x,y) = \dots = \alpha\mathbb{E}(X) + \beta\mathbb{E}(Y) + \gamma
\]

Can be generalized to: $\mathbb{E}(\alpha_1z_1 + \alpha_2z_2 + \dots +
\alpha_nz_n + \beta) = \alpha_1 \mathbb{E}(z_1) + \alpha_2
\mathbb{E}(z_2) + \dots + \alpha_n \mathbb{E}(z_n) + \beta$

\subsubsection{Conditional pmf}

Given $\Omega, \mathbb{P}$, a discrete rv $X$ defined on $\Omega$, an
event $A \subset \Omega, \mathbb{P}(A) > 0$, and a possible value
$x \in X$, the \textbf{conditional pmf} of $X$ given $A$ is defined
as:

\begin{equation}
  \tag{Conditional pmf on event}
  \boxed{
    p_{X\mid A}(x) = \frac{\mathbb{P}(\{X = x\} \cap A)}
    {\mathbb{P}(A)} = "\mathbb{P}(B \mid A) \text{ where } B \text{ is
      the event } \{X=x\}"
  }
\end{equation}

Observe that for any $A$ with $\mathbb{P}(A) > 0, p_{X\mid A}(x)$ as
$x$ ranges over $X$'s values defines a pmf: i.e. $p_{X\mid A}(x) \leq
0, \  \forall x$ and $\sum_{x \in X} p_{X\mid A}(x) = 1$.

\subsubsection{RVs Conditional on RVs}

Given $X,Y$ defined on some $\Omega, \mathbb{P}$, conditional pmf of
$X$ given $Y$ is defined on $\forall x \ \& \ \forall y$, with
$\mathbb{P}(\{Y=y\}) = p_Y(y) > 0$ as:

\begin{equation}
  \tag{Conditional pmf on rv}
  \boxed{
    p_{X\mid Y}(x\mid y) = \frac{\mathbb{P}(\{X=x\} \cap \{Y=y\})}
    {\mathbb{P}(\{Y=y\})} = \frac{p_{X,Y}(x,y)}{p_Y(y)}
  }
\end{equation}

Observe that for any fixed $y$ with $p_Y(y) > 0, p_{X\mid Y}(x\mid y)$
as $x$ ranges over $X$ values defines a pmf: i.e. $p_{X\mid Y} \geq 0$
and $\sum_{x \in X} p_{X\mid Y}(x\mid y) = 1$.


\medskip\hrule
\subsection{Lecture 12 - Conditional Probability for RV's, Conditional
  Expectation}

\subsubsection{Conditional Probability}

Given events $A_1, A_2, \dots, A_n$ that partition $\Omega$ and
$\mathbb{P}(A_k) > 0,\ 0 \leq k \leq n$, then for any discrete rv $X$ on
$\Omega$,

\begin{equation}
  \tag{Conditional Total Probability}
  \boxed{
    p_X(x) = \sum_{k=1}^n p_{X \mid A_k} \mathbb{P}(A_k)
  }
\end{equation}

There are also ways of expressing the joint pmfs in terms of the
marginals and vice versa:

\begin{equation}
  \tag{Product Rule of Sorts}
  \boxed{
    \begin{aligned}
      p_{X,Y}(x,y)
      &= p_Y(y) p_{X\mid Y}(x\mid y)
      && \forall x \in X
      &&& \text{ or,} \\
      p_{X,Y}(x,y)
      &= p_X(x) p_{Y\mid X}(y\mid x)
      && \forall y \in Y
    \end{aligned}
  }
\end{equation}

\begin{equation}
  \tag{Total-Prob Rule of Sorts}
  \boxed{
    \begin{aligned}
      p_X(x)
      &= \sum_{y \in Y} p_Y(y) p_{X\mid Y}(x \mid y)
      && \forall x \in X
      &&& \text{ or,} \\
      p_Y(y)
      &= \sum_{x \in X} p_X(x) p_{Y \mid X} (y\mid x)
      && \forall y \in Y
    \end{aligned}
  }
\end{equation}

These also generalize to $> 2$ rvs:

\[
  p_{X\mid Y,Z}(x\mid y,z) = \frac{\mathbb{P}(\{X=x\} \cap \{Y=y\}
    \cap \{Z = z\})} {\mathbb{P}(\{Y=y\} \cap \{Z=z\})} =
  \frac{p_{X,Y,Z} (x,y,z)} {p_{Y,Z}(y,z)}
\]

\[
  p_{X,Y,Z}(x,y,z) = p_Z(z) p_{Y \mid Z}(y\mid z) p_{X\mid Y,Z} (x
  \mid y,z)
\]

\subsubsection{Conditional Expectation}

Given $\Omega, \mathbb{P}$, a discrete rv $X$, and an event $A$, we
define the \textbf{Conditional Expectation} of $X$ given event $A$ as:

\begin{equation}
  \tag{Conditional Expectation}
  \boxed{
    \mathbb{E}(X \mid A) = \sum_{x\in X} xp_{X\mid A}(x)
  }
\end{equation}

We know that given events $A_1, \dots, A_n$ that partition $\Omega$,
$p_X(x) = \sum_{k=1}^n p_{X\mid A_k} (x\mid A_k)
\mathbb{P}(A_k)$. From this we can derive:

\[
  \mathbb{E}(X) = \sum_{x\in X} xp_X(x) = \sum_x \sum_k xp_{X\mid A_k}
  (x) \mathbb{P}(A_k) = \sum_k(\sum_x xp_{x\mid A_k}(x))
  \mathbb{P}(A_k)
\]

Given $\Omega, \mathbb{P}$, rvs $X,Y$, event $A = \{Y=y\}$:

\[
  \mathbb{E}(X\mid A) = \sum_{x\in X} xp_{X\mid A}(x) = \sum_{x\in X}
  xp_{X\mid A}(x \mid y)
\]

Since all events $\{Y = y\}$ partition $\Omega$, we get:

\[
  \mathbb{E}(X) = \sum_{y\in Y} \mathbb{E}(X \mid Y=y)
  \mathbb{P}(\{Y=y\})
\]


\medskip\hrule
\subsection{Lecture 13 \& 14 - Indpendence of RVs}

Given $\Omega, \mathbb{P}$, a discrete rv $X$ defined of $\Omega$, and
an event $A$, say $X$ is \textbf{independent (event)} of $A$ when
every event $\{X=x\}$ is independent of $A$ (event-wise), i.e:

\begin{equation}
  \tag{RV Event Independence}
  \boxed{
    \mathbb{P}(\{X=x\} \cap A) = p_X(x) \mathbb{P}(A)
  }
\end{equation}

Note that when $\mathbb{P}(A) > 0$, it is the same as stating
$p_{X\mid A} (x) = p_X(x),\ \forall x$.

Say two rvs $X$ and $Y$ are \textbf{independent (rvs)} when:

\begin{itemize}
\item $X$ is indpendent of every event $\{Y=y\}$.
\item $Y$ is independent of every event $\{X=x\}$.
\item $p_{X,Y}(x,y) = p_X(x) p_Y(y) \quad \forall x\in X, y\in Y$.
\end{itemize}

This extends to multiple rvs $X_1\dots X_n$. These rvs are independent
when:

\[
  \underbrace{p_{X_1,\dots,X_n}(x_1,\dots,x_n)}_{\text{joint}} =
  \underbrace{p_{X_1}(x_1)\dots p_{X_n}(X_n)}_{\text{product of
      marginals}} \quad \forall x_1, \dots, x_n
\]

Important facts about rv independence:

\begin{itemize}
\item If $X, Y$ are indpendent, then $\mathbb{E}(XY) = \mathbb{E}(X)
  \mathbb{E}(Y)$
  
\item If $X, Y$ are indpendent, then $Var(X+Y) = Var(X) + Var(Y)$.  
\end{itemize}


\medskip\hrule
\subsection{Lecture 15 - ``Randomness'' of RVs}

\subsubsection{Binary Entropy}

We can quantify the ``randomness'' of an rv through the use of binary
entropy. Suppose an rv $X$ has $N$ possible values, and that $p_X(x_k)
= p_k, 1\leq k\leq N$, ($p_K=0$ is allowed). We define the
\textbf{binary entropy} of discrete rvs as:

\begin{equation}
  \tag{Binary Entroy of RVs}
  \boxed{
    H(X) = \sum_{k=1}^{N} p_klog_2(p_k)
  }
\end{equation}

$H(X)$ then becomes a good measure of ``how random $X$ is''.

\bigskip
\underline{Important Facts About Entropy:}

\begin{itemize}
\item If $p_k = 1$ for some $k$ and $0$ for all other $k$, $X$ is
  ``least random''.

\item If $p_k = \frac{1}{N} \forall k, X$ is ``maximally random''.
  
\item $0 \leq H(X) \leq log_2N$.
  
\item $H(X) = 0 \Leftrightarrow p_k = 1$ for some $k$.
\end{itemize}

\subsubsection{Source Coding Theorem}

Given $X$ with pmf $p_X(x_k) = p_k, 1 \leq k \leq N$, any guaranteed
successful Y/N 20 questions scheme for determining value of $X$ has a
mean number of questions $\mathbb{E}(L)$ such that:

\begin{equation}
  \tag{Source Coding Theorm}
  \boxed{
    \mathbb{E}(L) \leq H(X)
  }
\end{equation}

We compute $\mathbb{E}(L)$ as follows: let $l_k = \#(\text{questions
  you need to ask when } X = x_k)$, then:

\[
  \mathbb{E}(L) = \sum_{k=1}^Np_kl_k
\]


\medskip\hrule
\subsection{Lecture 16 - Continuous Random Variables}

\subsubsection{Continous Random Variables}

Given $\Omega, \mathbb{P}, X: \Omega \rightarrow \mathbb{R}$ is a
\textbf{continous rv} when there's a ``reasonable'' function $f_X(x)$
such that for every $V \subset \mathbb{R}$, we have:

\begin{equation}
  \tag{Continuous rv}
  \boxed{
    \mathbb{P}(\{X\in V\}) = \int_V f_X(x)dx
  }
\end{equation}

That function $f_X(x)$ is called the \textbf{probability density
  function (pdf)} of $X$. It can be interpreted as the ``probability
'mass' per unit 'length''' of an rv.

Special Case of $V$: $V = [a,b]$ or $[a,b), (a,b], (a,b)$ we have:

\[
  \mathbb{P}(\{X\in V\}) = \int_a^b f_X(x)dx
\]

Some properties of $f_X(x)$:

\begin{itemize}
\item $f_X(x) \geq 0 \ \forall x$ (need to ensure $\mathbb{P}(\{X \in
  V\}) \geq 0$ for all $V \subset R$)

\item $\lim\limits_{R\to\infty} \int\limits_{-R}^{+R} f_X(x) dx = 1
  \rightarrow \int\limits_{-\infty}^{\infty} f_X(x)dx =
  \mathbb{P}(\{X\in(-\infty,\infty)\}) = 1$

\item Given $x\in R$, $f_X(x)$ is NOT $\mathbb{P}(\text{some event})$
  --- in particular, $f_X(x) \not= \mathbb{P}(\{X=x\})$.

\item Turns out $\mathbb{P}(\{X=x\})=0 \quad \forall x\in R$ when $X$
  is a continuous random variable.

\item Since $f_X(x)$ isn't $\mathbb{P}(\text{some event})$, need not
  have $f_X(x) \leq 1$! In fact, $f_X(x)$ can take on arbitrarily
  large values!
\end{itemize}

\subsubsection{Expected Value}

The \textbf{expected value} of a continous rv $X$ with pdf $f_X(x)$ is
defined as (Note: expected value not always defined, integral might
fail to exist):

\begin{equation}
  \tag{Expected Value: Continous rv}
  \boxed{
    \mathbb{E}(X) = \int_{-\infty}^{+\infty} xf_X(x)dx
  }
\end{equation}

Given continous rv $X$ with pdf $f_X(x)$ and $Y=g(X)$, we define the
\textbf{Expected Value Rule} as:

\begin{equation}
  \tag{Expected Value Rule}
  \boxed{
    \mathbb{E}[Y] = \int_{-\infty}^{+\infty} g(X) f_X(x) dx
  }
\end{equation}

Special Case: $g(X) = \alpha X + \beta$

\[
  \mathbb{E}[g(X)] = \alpha \mathbb{E}[X] + \beta
\]

\subsubsection{Variance}

Given continous rv $X$ w/ defined expected value $\mathbb{E}[X]$, we
define the \textbf{Variance} as:

\begin{equation}
  \tag{Variance: Continous rv}
  \boxed{
    Var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]
  }
\end{equation}

By expected value rule, we also have:

\[
  Var(X) = \int_{-\infty}^\infty (X - \mathbb{E})^2 f_X(x) dx
\]

Also, as before:

\[
  Var(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\]

\subsubsection{Interpretations}

$\mathbb{E}(X)$ can interpreted as the center of the ``probability
mass'' defined by $f_X(x)$.

$Var(X)$ can be interpreted as the spread of the ``proability mass''
about its center.

\subsubsection{Common Continuous RVs}

Some common continuous rvs are:

\begin{itemize}
\item \textbf{X uniform on [a,b]:} Given $a,b \in \mathbb{R};\ a < b$;
  let the pdf of the ``uniform on [a,b]'' rv be defined as:
  
  \[
    f_X(x) =
    \begin{cases}
      \frac{1}{b-a} & \text{when } x \in [a,b] \\
      0 & \text{else}
    \end{cases}
  \]

  For the uniform on [a,b] rv, we have: $\mathbb{E}[X] = \frac{b+a}{2}$
  and $Var(X) = \frac{(b-a}^2}{12}$.

\item \textbf{X exponential($\lambda$):} Given $\lambda \in
  \mathbb{R}$; let the pdf of the ``exponential($\lambda$)'' rv be
  defined as:
  
  \[
    f_X(x) =
    \begin{cases}
      \lambda e^{-\lambda x} & \text{when } x \geq 0 \quad \forall
      \lambda > 0 \\
      0 & \text{when } x < 0
    \end{cases}
  \]

  For the exponential($\lambda$) rv, we have: $\mathbb{E}[X] =
  \frac{1}{\lambda}$ and $Var(X) = \frac{1}{\lambda^2}$.

\item \textbf{X piecewise uniform:} TODO
\end{itemize}



\medskip\hrule
\subsection{Lecture 17 - Cumulative Distribution Function}

\subsubsection{Cumulative Distribution Function}

For any rv $X$ (discrete of continous), the \textbf{cumulative
  distribution function (cdf)} if defined as:

\begin{equation}
  \tag{Cdf Definition: Continuous}
  \boxed{
    F_X(x) = \mathbb{P}(\{X\leq x\}) \quad \forall x \in \mathbb{R}
  }
\end{equation}

If $X$ is a continuous rv w/ pdf $f_X(x)$, then since
$\mathbb{P}(\{X\leq x\}) = \int_{-\infty}^x f_X(t)dt$, we have:

\[
  F_X(x) = \int_{-\infty}^x f_X(t)dt \quad \text{ and } \quad
  f_X(x) = \frac{d}{dx} F_X(x)
\]

\underline{Discrete Version:} If $X$ is a discrete rv with pmf $p_X(x)$,
we have:

\begin{equation}
  \tag{Cdf Definition: Discrete}
  \boxed{
    F_X(x) = \sum_{\{x_k \mid x_k \leq x\}} p_X(x_k)
  }
\end{equation}

This formula can also be inverted to get $p_X(x)$ in terms of
$F_X(x)$:

\[
  p_X(x_k) = F_X(x_k) - F_X(x_{k-1})
\]

where $x_{k-1}$ is the ``next largest value'' of $X$ below $x_k$.

\subsubsection{General Properties of cdfs}

\begin{enumerate}
\item $\lim_{x\to-\infty}F_X(x) = 0$ and $\lim_{x\to\infty}F_X(x) = 1$.
  
\item When $X$ is a continuous rv, $F_X(x)$ is continuous in $x$ and
  differentiable ``almost everywhere'' (corners in $F_X(x)$ correspond
  to jump in $f_X(x)$)

\item $X$ is a discrete rv iff $F_X(x)$ is peice wise constant.

\item $F_X(x)$ is monotomically increasing in $x$, i.e.
  \[
    x_1 \leq x_2 \quad \Rightarrow \quad F_X(x_1) \leq F_X(x_2)
  \]
\end{enumerate}

Cdfs are alse useful for getting the pdf of $X$ by first computing
$F_X(x)$, then taking $d/dx$.

\subsubsection{Gaussian rv}

Another inportant continuous rv is the \textbf{Gaussian rv}. The pdf
of the Gaussian rv is:

\[
  f_X(x) = \frac{1} {\sqrt{2x\sigma^2}}
  \exp{\bigg(-\frac{(x-M)^2\bigg)} {2\sigma^2}}
\]

With this pdf, we can see that $\mathbb{E}(X) = M$ and $Var(X) =
\mathbb{E}((X-M)^2) = \sigma^2$. The cdf of a Gaussian rv is:

\[
  F_X(x) = \frac{1} {\sqrt{2x\sigma^2}} \int_{-\infty}^x
  \exp{-\frac{(t-m)^2}{2\sigma^2}} dt
\]

The \textbf{standard normal} pdf is a specific type of Gaussian pdf:

\[
  \frac{1}{\sqrt{2\pi}} e^{-x^2/2} \quad \therefore M=0, \ \sigma=1
\]

The cdf of the standard normal pdf is:

\[
  \Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-t^2/2} dt
\]

The standard normal pdf can be created from a Gaussian rv with mean
$M$ and var $\sigma^2$ as $Y=\frac{X-M}{\sigma}$ (Example):

\[
  \mathbb{P}(\{X>17\}) = \mathbb{P}(\{\sigma + Y + M > 17\}) =
  \mathbb{P}\Big(Y>\frac{17-M}{\sigma}\Big)
\]

Gaussian rvs are important because the sum of many different
independent rvs that have the same pdf ``converges'' to a Gaussian
rv.


\medskip\hrule
\subsection{Lecture 18 - Multiple Continous Random Variables}

\subsubsection{Joint Continuous Random Variables}

Say $X$, $Y$ rvs defined in some $\Omega$, $\mathbb{P}$ are
\textbf{jointly continuous} with \textbf{joint pdf} $f_{X,Y}(x,y)$
when:

\begin{equation}
  \tag{Jointly Continuous Definition}
  \boxed{
    \mathbb{P}(\{(X,Y)\in V\}) = \iint_{-V} f_{X,Y}(x,y) dx dy \quad
    \forall V \subset \mathbb{R}^2
  }
\end{equation}

Special Case of $V$: $V : [a_1,b_1] \times [a_2,b_2]$

\[
  \mathbb{P}(\{X,Y)\in V\}) = \int_{a_1}^{b_2}dx \int_{a_2}^{b_2}dy
  (f_{X,Y}(x,Y))
\]

From the joint pdf, we can also derive the marginals:

\begin{equation}
  \tag{Joint pdf Marginals}
  \boxed{
    \begin{split}
      f_X(x) &= \int_{-\infty}^{+\infty} f_{X,Y}(x,y) dy \\
      f_Y(y) &= \int_{-\infty}^{+\infty} f_{X,Y}(x,y) dx
    \end{split}
  }
\end{equation}

Other Properties:

\begin{itemize}
\item $\int\limits_{-\infty}^{+\infty} dx
  \int\limits_{-\infty}^{+\infty} dy (f_{X,Y}(x,y)) = 1$

\item Joint CDF: $F_{X,Y}(x,y) = \mathbb{P}(\{X=x\} \cap \{Y=y\}) =
  \int\limits_{-\infty}^{x} ds \int\limits_{-\infty}^{y} dy
  (f_{X,Y}(s,t))$

\item $f_{X,Y}(x,) = \frac{\delta}{\delta x} \frac{\delta}{\delta y}
  F_{X,Y}(x,y)$

\item Generalization to $> 2$ rv pretty ``straightforward''
\end{itemize}

For discrete rvs, joint determines marginals, but not vice-versa.

\subsubsection{Conditional for Continuous rvs: Events}

Given a continuous rv $X$ on $\Omega$, $\mathbb{P}$, and some event
$A\subset\Omega$, the conditional pdf of $X$ given $A$ ``defined'' as
follows:

\begin{equation}
  \tag{Conditional pdf: Event}
  \boxed{
    \mathbb{P}(\{X\in V\}\mid A) = \int_V f_{X\mid A}(x) dx \quad
    \forall V \subset \mathbb{R}
  }
\end{equation}

In general, no decent formula for $f_{X\mid A}(x)$ in terms of
$f_X(x)$. One way to compute it is to take the conditional cdf of $X$
given $A$ ($F_{X\mid A} = \mathbb{P}(\{X \leq x\} \mid A)$) and take
$d/dx$ to find $f_X(x)$. However, if $A$ is an event of the form $\{X
\in W\}$ and $\mathbb{P}(A)>0$, we have:

\[
  f_{X\mid A}(x) =
  \begin{cases}
    \frac{f_X(x)} {\mathbb{P}(\{X\in W\})} & \text{when } X\in W \\
    0 & \text{otherwise}
  \end{cases}
\]

We derive this by defining the indication function of $W$ as:

\[
  \chi_W(x) =
  \begin{cases}
    1 & \text{when } x\in W \\
    0 & \text{when } x\not\in W
  \end{cases}
\]

And using conditional functions for continuous rvs:

\[
  \begin{aligned}
    \mathbb{P}(\{X\in W\}\mid A)
    &= \frac{\mathbb{P}(\{X\in (V\cap W)\})} {\mathbb{P}(\{x\in W\})} \\
    &= \frac{\int_{V\cap W}f_X(x)dx} {\mathbb{P}(\{X\in W\})} \\
    &= \int_V\bigg(\frac{f_X(x)\chi_W(x)} {\mathbb{P}(\{X\in W\})}\bigg) dx
    &&= \begin{cases}
      \frac{f_X(x)} {\mathbb{P}(\{X\in W\})} & \text{when } X\in W \\
      0 & \text{otherwise}
    \end{cases}
  \end{aligned}
\]


\medskip\hrule
\subsection{Lecture 19 - Total Probability Theorem}

\subsubsection{Total Probability Theorem}

\subsubsection{Conditional for Continuous rvs: Other rvs}


\medskip\hrule
\subsection{Lecture 20 - Conditional Expectance, Independence,
  Continuous Bayes' Rule}

\subsubsection{Conditional Expected Value}

\subsubsection{Indpendence}

For any pair $X,Y$, both continuous rvs with densities $f_X(x)$,
$f_Y(y)$, $f_{X,Y}(x,y)$, $X$ and $Y$ are independent iff:

\begin{equation}
  \tag{Independence: Continuous}
  \boxed{
    f_{X,Y}(x,y) = f_X(x)f_Y(y) \quad \forall x, y
  }
\end{equation}

Note: When $X$ and $Y$ are independent, we have:

\begin{itemize}
\item $\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$
\item $\mathbb{E}(g(X)h(Y)) = \mathbb{E}(g(X))\mathbb{E}(h(Y))$
\item $Var(X+Y) = Var(X)+Var(Y)$
\end{itemize}

\subsubsection{Continuous Bayes' Rule}


\medskip\hrule
\end{document}