VG\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper,top=3cm,bottom=2cm,left=3cm,right=3cm,
marginpaperwidth=1.75cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{titlesec}

% \setlength{\parskip}{1em} 
\titlespacing*{\subsection}
  {0pt}{3\baselineskip}{\baselineskip}

\title{ECE 3100 - Functions, Formulas, and Definitions}
\author{Stephen Chin}
\date{Spring Semester 2019}


\begin{document}
\maketitle

\section{Pre - Prelim 1}

\subsection{Lecture 1 - What is Probability?}

\textbf{Probability} is a way of mathematically modelling situations
involving uncertainty with the goal of making predications decisions
and models. Probability can be understood in many ways, such as:

\begin{enumerate}
\item Frequency of Occurence: Or percentage of successes in a
  moderately large number of similar situations.
  
\item Subjective belief: Or ceratinty based on other understood facts
  about a claim.
\end{enumerate}

For our Probability Models, we define the set of all outcomes to be
$\Omega$, better known as the \textbf{sample space} of an
experiment. All subsets of $\Omega$ are called \textbf{events}. These
are both sets and can be understood using default set notation.


\subsection{Lecture 2 - Probability Law}

Given $\Omega$ chosen, a \textbf{probability law} on $\Omega$ is a
mapping $\mathbb{P}$ that assings a number for every event such that:

\begin{equation} \tag{Kolmogorov's Axioms} \boxed{
    \begin{aligned} \mathbb{P}(A) \ge 0 & \quad \text{for every event
        A} \\ \mathbb{P}(\Omega) = 1 & \quad \text{(normalization)}
    \end{aligned} }
\end{equation}

\bigskip
\subsubsection{Additivity rules:}

\begin{itemize}
\item If $A \cap B = \varnothing$, ($A, B$) events, then:
  \begin{equation}
    \boxed{
      \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B)
    }
  \end{equation}

\item If events $A_1, A_2, \dots$ are all disjoint, then:
  \begin{equation}
    \boxed{
      \mathbb{P} (\bigcup\limits_{n=1}^{\infty} A_n) =
      \sum_{n=1}^{\infty} \mathbb{P}(A_n)
    }
  \end{equation}
\end{itemize}

By these rules, we can surmise that $\boxed{\mathbb{P} (\varnothing) =
  0}$.

For any events $A, B$:
\begin{equation}
  \tag{Event Union}
  \boxed{
    \mathbb{P}(A \cup B) = \mathbb{P}(A) + \mathbb{P}(B) -
    \mathbb{P}(A \cap B)
  }
\end{equation}

When we have a probability law on a finite $\Omega$ with all outcomes
equally likely (i.e. $\mathbb{P}(\{s\}) = 1/size(\Omega)$), we call
this probability law $\mathbb{P}$ a \textbf{(discrete) uniform
  probability law}.


\subsection{Lecture 3 - Conditional Prob \& Product Rule}

\subsubsection{Conditional Probability}

\textbf{Conditional Probability} is defined $\mathbb{P}(A \mid B)$ =
``Probability of A given B''. It is understood as the likelyhood that
event A occurs, given that B also occurs. 

\begin{equation}
  \tag{Conditional Probability Def}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
  }
\end{equation}

If there is a finite number of different outcomes that are all equally
likely, the conditional prbability can be written as follows:

\begin{equation}
  \boxed{
    \mathbb{P}(A \mid B) = \frac{\text{number of elements of } A \cup
      B}{\text{number of elements of } B}
  }
\end{equation}

\subsubsection{Product Rule}

There are two main ways to write the product rule and they both have
different setups.

\begin{itemize}
\item If we have events $D_1$ to $D_n$ where $D_1 > D_2 > \dots > D_n$
  ($D_1$ largest, $D_n$ smallest), then we can apply the first form of
  the product rule:

  \begin{equation}
    \tag{Product Rule 1}
    \boxed{
      \mathbb{P}(D_n) = \mathbb{P}(D_1) \mathbb{P}(D_2 \mid D_1)
      \mathbb{P}(D_3 \mid D_2) \dots \mathbb{P}(D_n \mid D_{n-1}
    }
  \end{equation}

\item If we have events $A_1$ to $A_n$ with non-empty intersection
  (i.e. $A_1 \cap A_2 \cap \dots \cap A_n$), let $D_k = A_1 \cap A_2
  \cap \dots \cap A_k$, then $D_1 > D_2 > \dots > D_n$. If we then
  write the product rule on the events $D_n$ in terms of $A_n$, we
  get:

  \begin{equation}
    \tag{Product Rule 2}
    \boxed{
      \mathbb{P}(A_1 \cap \dots \cap A_n) = \mathbb{P}(A_1)
      \mathbb{P}(A_2 \mid A_1) \mathbb{P}(A_3 \mid A_1 \cap A_2) \dots
      \mathbb{P}(A_n \mid A_1 \cap \dots)
    }
  \end{equation}
\end{itemize}


\subsection{Lecture 4 - Total Probability}

Given an event $B$, say $C_1, C_2, \dots, C_n$ (events) is a
\textbf{partition} of $B$ when:
\begin{itemize}
\item $B = C_1 \cup C_2 \cup \dots \cup C_n$
\item $C$'s are all disjoint
\end{itemize}

If $A_1, A_2, \dots, A_n$ is a partition of $\Omega$, then $C_1, C_2,
\dots, C_n$ partitions $B$, where $C_k = B_k A_k$ for $a \leq k \leq
n$.

From there we define the Total Probability Theorem:

\begin{equation}
  \tag{Total Probability Theorem}
  \boxed{
    \mathbb{P}(B) = \mathbb{P}(B \mid C_1) \mathbb{P}(C_1) +
    \mathbb{P}(B \mid C_2) \mathbb{P}(C_2) + \dots + \mathbb{P}(B \mid
    C_n) \mathbb{P}(C_n)
  }
\end{equation}


\subsection{Lecture 5 - Bayes Law \& Independence}

\subsubsection{Bayes Law}

\textbf{Bayes' Rule} is defined by mixing the defintion of Condition
Probability, and the Total Probability Theorem.

Given $\Omega, \mathbb{P}$, if $A_1, A_2, \dots, A_n$ are events that
partition $\Omega$, and have nonzero $\mathbb{P}(A)$, then for any
event $B$,

\begin{equation}
  \tag{Bayes' Law}
  \boxed{
    \mathbb{P}(A_k \mid B) =
    \frac{\mathbb{P}(B \mid A_k) \mathb{P}(A_k)}
    {\mathbb{P}(B \mid A_1) \mathbb{P}(A_1) + \dots + \mathbb{P}(B
      \mid A_n)}
  }
\end{equation}

\subsubsection{Indpendence}

Given $\Omega, \mathbb{P}$, any events $A$ and $B$ are
\textbf{independent} when:

\begin{equation}
  \tag{Independence Def}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B)
      &= \mathbb{P}(A) \mathbb{P}(B)
      && \text{or} \\
      \mathbb{P}(A \mid B)
      &= \mathbb{P}(A), \mathbb{P}(B) > 0
      && \text{or} \\
      \mathbb{P}(B \mid A)
      &= \mathbb{P}(B), \mathbb{P}(A) > 0
    \end{aligned}
  }
\end{equation}


\subsection{Lecture 6 - Conditional Dependence \& Counting}

\subsubsection{Conditional Dependence}

Given $\Omega$ and $\mathbb{P}$: say that events $A$ and $B$ are
\textbf{conditionally independent} given $C$ when:

\begin{equation}
  \tag{Conditional Independence Def}
  \boxed{
    \begin{aligned}
      \mathbb{P}(A \cap B \mid C)
      &= \mathbb{P}(A \mid C) \mathbb{P}(B \mid C)
      && \text{or} \\
      \mathbb{P}(A \mid B \cap C)
      &= \mathbb{P}(A), \mathbb{P}(B) > 0
      && \text{or} \\
      \mathbb{P}(B \mid A)
      &= \mathbb{P}(B), \mathbb{P}(A) > 0
    \end{aligned}
  }
\end{equation}

\subsubsection{Counting}

\textbf{Counting} is the process of using the number of elements in
the events to calculate probability. This technique mostly arises in
situations where either:

\begin{itemize}
\item The sample space $\Omega$ has a finite number of equally likely
  outcomes. Then, for any event $A$,

  \[
    \mathbb{P}(A) = \frac{\text{\# of elements of } A}
    {\text{\# of elements of } \Omega}
  \]

\item An event $A$ has a finite number of equally likely outcomes with
  probability $p$. Then for that event $A$:

  \[
    \mathbb{P}(A) = p \cdot (\text{\# of elements of }A)
  \]
\end{itemize}


\subsection{Lecture 7 - Counting}

\textbf{Counting Principle:} in a process with a sequence of stages
$1, 2 \dots, r$ with $n_1$ choices at stage 1 over to $n_r$ at stage
$r$; \# of coutcomes is $n_1 n_2 \dots n_k$.

Can be used to rederive (\# subsets of $\Omega$) = $2^{\#(elem)}$.

\bigskip
\subsubsection{$k$-permutations of $n$ objects}

We are given $n$ distinct objects and a number $k \leq n$, and we want
to find out the number of ways we could take $k$ distinct objects from
the group of $n$ objects and arrange them in a sequence. By using the
Counting Principle, we can find that the \textbf{number of
  k-permutations} of this set is:

\begin{equation}
  \tag{K-permutations}
  \boxed{
    \begin{aligned}
      n (n-1) \dots (n-k+1)
      &= \frac{n (n-1) \dots (n-k+1) (n-k) \dots 2 \cdot 1}{(n-k)
        \dots 2 \cdot 1} \\
      &= \frac{n!}{(n-k)!}
    \end{aligned}
  }
\end{equation}

Special Case: If $k = n$, then the number of $k$-permutations of $n$
objects is simply $n!$.

\bigskip
\subsubsection{$k$-combinations of $n$ objects}

For finding the number of $k$-combinations, we can look back to our
$k$-permutations and reason about them. Say we have the same setup as
before but we are not arranging the items in a sequence. For each
combination, we have $k!$ ``duplicate'' permutations. Thus, we can
look at the number permutations and reason that the number of
k-combinations should be that over $k!$, making the \textbf{number of
  k-combinations} of this set is:

\begin{equation}
  \tag{K-combinations}
  \boxed{
    \frac{n!}{k!(n-k)!} = \binom{n}{k}
  }
\end{equation}


\subsection{Lecture 8 - Discreete Random Variables}

\subsubsection{Random Variables}

Given $\Omega$ and $\mathbb{P}$, a \textbf{discrete random variable
  (r.v.)} is a real valued function with domain $\Omega$ that takes on
only finite or countably infinite number of different values (i.e. $X
: \Omega \rightarrow \mathbb{R}$).

\subsubsection{Probability Mass Functions}

Given $\Omega, \mathbb{P}$, associated with any discrete rv $X :
\Omega \rightarrow \mathbb{R}$ is $X$'s \textbf{probability mass
  function (pmf)} - notation $p_X$.

\begin{equation}
  \tag{pmf Def}
  \boxed{
    \forall x \text{ of } X, p_X(x) = \mathbb{P}(A_X)
    \text{ where } A_X = \{s \in \Omega : X(s) = x\}
  }
\end{equation}

\bigskip
\underline{Things to Note:}

\begin{itemize}
\item $\mathbb{P}(A_X)$ can also be written as $\mathbb{P}(\{X = x\})$
  or $\mathbb{P}(X = x)$.

\item $p_X(x) \geq 0$ for all possible values of $X$.

\item A pmf is essentially a probability law on the different values
  in the codomain of a random variable, so the same laws that apply
  to probability laws apply to pmfs:

  \[
    \begin{aligned}
      p_X(x) \ge 0 & \quad \text{for every } x \in X \\
      \sum\limits_{x \in X} p_X(x) = 1 & \quad \text{(normalization)}
    \end{aligned}
  \]

\item If $V$ is any finite or countably inf. set of possible values of
  $X$, then if we set $B = \{ $the event ``$X \in V$'' $\}$,
  (i.e. $B = \{ s \in \Omega : X(s) \in V \} $), then
  $\mathbb{P}(B) = \sum\limits_{x \in V} p_X(x)$.
\end{itemize}

Note: for a given pmf, there are multiple $\Omega$'s, $\mathbb{P}$'s,
$X$'s that lead to that PMF.

\subsubsection{Common PMFs}

\begin{itemize}
\item \textbf{Discrete uniform pmf of interval $a \leq k \leq b$},
  $a, b \in \mathbb{N}$:
  
  \[
    p_X(k) =
    \begin{cases}
      \frac{1}{b - a + 1} & \text{when } a \leq k \leq b \\
      0 & \text{all over } k
    \end{cases}
  \]

\item Let $p \in [0,1]$; the \textbf{Bernoulli p pmf} be defined by:
  
  \[
    p_X(k) =
    \begin{cases}
      p & \text{when } k = 1 \\
      1 - p & \text{when } k = 0
    \end{cases}
  \]
    
\item Given positive integer $n$, some $p \in [0,1]$, the
  \textbf{Binomial(n,p) pmf} is defined as:

  \[
    p_X(k) = {n \choose k} p^k (1 - p)^{n-k} \quad 0 \leq k \leq n
  \]

  This pmf tends to show up in situations involving sequences of
  independent trials, such as coin flips. Useful if you are trying to
  find the probability of $k$ heads in $n$ coin flips.

\item Given $p \in (0, 1)$ the \textbf{geometric pmf} defined by:

  \[
    p_X(k) = p (1-p)^{k-1} \quad \text{for all } 1 \leq k \leq \infty
    \text{positive integers}
  \]

  This pmf tends to show up in situations such as $\mathbb{P}$(it
  takes $k$ flips to flip a heads).

\item Poisson(X):

  \[
    p_X(k) = e^{-\lambda} \frac{\lambda^{k}}{k!} \quad 0 \leq k \leq
    \infty (k \in \mathbb{N})
  \]
\end{itemize}

\subsection{Lecture 9 - Expectation, Variance}

\subsubsection{Function of a Random Variable}

Given a random variable $X$ and any function $g : \mathbb{R}
\rightarrow \mathbb{R}$, can define another r.v. $Y = g(x)$:

\[
  \forall s \in \Omega, Y(s) = g(X(s))
\]

The function of a discrete r.v. is another discrete r.v.. Generally,
it is non-trivial to get the pmf of $Y=g(X)$, but it is sometimes
easy. (See examples)

\subsubsection{Expected Value}

Given a discrete r.v. $X$ with $p_X(x)$ pmf, we define the
\textbf{expected value (expectation)}:

\begin{equation}
  \tag{Expected Value Definition}
  \boxed{
    \mathbb{E}(X) = \sum\limits_{x \in X} x p_X(x)
  }
\end{equation}

Given $X, Y = g(X)$, what is $\mathbb{E}(Y)$? One way is to figure out
$p_Y(g)$ for all possible values of $y \in Y$ and then find it
through:

\[
  \mathbb{E}(Y) = \sum\limits_{y \in Y} y p_Y(y)
\]

and get $P_y$ through $p_x$, though that is generally a non-trivial
solution.

Another possible solution is to use the Expected Value Rule.

\subsubsection{Expected Value Rule}

Given $X, p_X, and Y=g(X)$,

\begin{equation}
  \tag{Expected Value Rule}
  \boxed{
    \mathbb{E}(Y) = \sum\limits_{x \in X} g(X) p_X(x)
  }
\end{equation}

Special Case: $Y = \alpha X + \beta$

\[
  \begin{aligned}
    \mathbb{E}(Y)
    &= \sum\limits_{x \in X} g(x) p_X(x) \\
    &= \sum\limits_{x \in X} (\alpha x + \beta) p_X(x) \\
    &= \alpha \sum\limits_{x \in X} x p_X(x) + \beta \sum\limits_{x
      \in X} p_X(x) \\
    &= \alpha \mathbb{E}(X) + \beta
    
  \end{aligned}
\]

\end{document}